{"cells": [{"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "3d01e43162b64c90ce0048e8a23f3b1b", "grade": false, "grade_id": "cell-f8987996be9f1238", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "# Accidentes de tr\u00e1fico en Reino Unido entre 2010 y 2014 \n\n### Disponible en Kaggle en:\nhttps://www.kaggle.com/stefanoleone992/adm-project-road-accidents-in-uk"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "9a6b4dc108ddf890c659e33701965428", "grade": false, "grade_id": "cell-f74d7bfd01811789", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Variables y significado"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "4a5a5882319ae0a14393c8d534816a56", "grade": false, "grade_id": "cell-9cfb34982bd4eb04", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "* Accident_Index: Accident index\n* Latitude: Accident latitude\n* Longitude: Accident longitude\n* Region: Accident region\n* Urban_or_Rural_Area: Accident area (rural or urban)\n* X1st_Road_Class: Accident road class\n* Driver_IMD_Decile: Road IMD Decile\n* Speed_limit: Road speed limit\n* Road_Type: Road type\n* Road_Surface_Conditions: Road surface condition\n* Weather: Weather\n* High_Wind: High wind\n* Lights: Road lights\n* Datetime: Accident datetime\n* Year: Accident year\n* Season: Accident season\n* Month_of_Year: Accident month\n* Day_of_Month: Accident day of month\n* Day_of_Week: Accident day of week\n* Hour_of_Day: Accident hour of day\n* Number_of_Vehicles: Accident number of vehicles\n* Age_of_Driver: Driver age\n* Age_of_Vehicle: Vehicle age\n* Junction_Detail: Accident junction detail\n* Junction_Location: Accident junction location\n* X1st_Point_of_Impact: Vehicle first point of impact\n* Driver_Journey_Purpose: Driver journey purpose\n* Engine_CC: Vehicle engine power (in CC)\n* Propulsion_Code: Vehicle propulsion code\n* Vehicle_Make: Vehicle brand\n* Vehicle_Category: Vehicle brand category\n* Vehicle_Manoeuvre: Vehicle manoeuvre when accident happened\n* Accident_Severity: Accident severity"}, {"cell_type": "markdown", "metadata": {}, "source": "**Nombre completo del alumno:**  **Tamara Infante Quintanar**"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "c13564336e237f236f7784bacbcc8e23", "grade": false, "grade_id": "cell-b4f9c37a2b92d2e6", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "# INSTRUCCIONES \n\nEn cada celda debes responder a la pregunta formulada, asegur\u00e1ndote de que el resultado queda guardado en la(s) variable(s) que por defecto vienen inicializadas a `None`. No se necesita usar variables intermedias, pero puedes hacerlo siempre que el resultado final del c\u00e1lculo quede guardado exactamente en la variable que ven\u00eda inicializada a None (debes reemplazar None por la secuencia de transformaciones necesarias, pero nunca cambiar el nombre de esa variable). \n\n**No olvides borrar la l\u00ednea *raise NotImplementedError()* de cada celda cuando hayas completado la soluci\u00f3n de esa celda y quieras probarla**.\n\nDespu\u00e9s de cada celda evaluable ver\u00e1s una celda con c\u00f3digo. Ejec\u00fatala (no modifiques su c\u00f3digo) y te dir\u00e1 si tu soluci\u00f3n es correcta o no. Adem\u00e1s de esas pruebas, se realizar\u00e1n algunas m\u00e1s (ocultas) a la hora de puntuar el ejercicio, pero evaluar dicha celda es un indicador bastante fiable acerca de si realmente has implementado la soluci\u00f3n correcta o no. Aseg\u00farate de que, al menos, todas las celdas indican que el c\u00f3digo es correcto antes de enviar el notebook terminado.\n\n**Nunca se debe redondear ninguna cantidad si no lo pide expl\u00edcitamente el enunciado**\n\n### Cada soluci\u00f3n debe escribirse obligatoriamente en la celda habilitada para ello. Cualquier celda adicional que se haya creado durante el desarrollo deber\u00e1 ser eliminada.\n\nSi necesitas crear celdas auxiliares durante el desarrollo, puedes hacerlo pero debes asegurarte de borrarlas antes de entregar el notebook. Las celdas auxiliares deben crearse siempre **debajo de las celdas de SOLUCI\u00d3N ya existentes**, o de lo contrario, Jupyter no te permitir\u00e1 borrarlas. Por tanto, en caso necesario, nunca debe crearse una celda nueva debajo de celdas de texto ni tampoco debajo de las celdas de autoevaluaci\u00f3n (assert), sino siempre debajo de las celdas de **soluci\u00f3n**."}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "e7764e6064699f591cd2896d2430528e", "grade": false, "grade_id": "cell-69ec0993eeaff3ac", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "### Sobre el dataset anterior (accidents_uk.csv) se pide:"}, {"cell_type": "markdown", "metadata": {}, "source": "**Ejercicio 1 (1.5 puntos)** \n* Leerlo tratando de que Spark infiera el tipo de dato de cada columna.\n* Crear una columna llamada `Age_Category` renombrando los valores de la columna `Age_of_Driver` donde los valores 1 y 2 de la columna original sean etiquetados en la columna nueva como \"Adolescente\", los valores 3 y 4 como \"Joven\", los valores 5 y 6 como \"Adulto\", y los valores 7 y 8 como \"Anciano\".\n* Crear una columna llamada `hora` aplicando la funci\u00f3n `F.hour` a la columna `\"Datetime\"` ya existente.\n* El resultado debe guardarse **cacheado** en la variable `accidentesDF`."}, {"cell_type": "code", "execution_count": 1, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "494ea1492132b3a5e88d7b7b5ea9c9ce", "grade": false, "grade_id": "read_csv", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- Accident_Index: string (nullable = true)\n |-- Latitude: double (nullable = true)\n |-- Longitude: double (nullable = true)\n |-- Region: string (nullable = true)\n |-- Urban_or_Rural_Area: string (nullable = true)\n |-- X1st_Road_Class: string (nullable = true)\n |-- Driver_IMD_Decile: integer (nullable = true)\n |-- Speed_limit: integer (nullable = true)\n |-- Road_Type: string (nullable = true)\n |-- Road_Surface_Conditions: string (nullable = true)\n |-- Weather: string (nullable = true)\n |-- High_Wind: string (nullable = true)\n |-- Lights: string (nullable = true)\n |-- Datetime: string (nullable = true)\n |-- Year: integer (nullable = true)\n |-- Season: integer (nullable = true)\n |-- Month_of_Year: integer (nullable = true)\n |-- Day_of_Month: integer (nullable = true)\n |-- Day_of_Week: integer (nullable = true)\n |-- Hour_of_Day: double (nullable = true)\n |-- Number_of_Vehicles: integer (nullable = true)\n |-- Age_of_Driver: integer (nullable = true)\n |-- Age_of_Vehicle: integer (nullable = true)\n |-- Junction_Detail: string (nullable = true)\n |-- Junction_Location: string (nullable = true)\n |-- X1st_Point_of_Impact: string (nullable = true)\n |-- Driver_Journey_Purpose: string (nullable = true)\n |-- Engine_CC: integer (nullable = true)\n |-- Propulsion_Code: string (nullable = true)\n |-- Vehicle_Make: string (nullable = true)\n |-- Vehicle_Category: string (nullable = true)\n |-- Vehicle_Manoeuvre: string (nullable = true)\n |-- Accident_Severity: string (nullable = true)\n |-- Age_Category: string (nullable = true)\n |-- hora: integer (nullable = true)\n\n"}], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\n# Leemos el dataset\naccidentesDF = spark.read.csv(\"gs://tamaramasterucm//accidents_uk.csv\", header=True, inferSchema=True)\n\nfrom pyspark.sql.functions import when, col, date_format, hour\nfrom pyspark.sql import SparkSession\n\n# Crear una sesi\u00f3n de Spark\nspark = SparkSession.builder.appName(\"EjemploPySpark\").config(\"spark.sql.debug.maxToStringFields\", 100).getOrCreate()\n\n# Crear las columnas Age_Category y hora\naccidentesDF = (\n    accidentesDF\n    .withColumn(\"Age_Category\",\n        when(col(\"Age_of_Driver\").between(1, 2), \"Adolescente\")\n        .when(col(\"Age_of_Driver\").between(3, 4), \"Joven\")\n        .when(col(\"Age_of_Driver\").between(5, 6), \"Adulto\")\n        .when(col(\"Age_of_Driver\").between(7, 8), \"Anciano\"))\n    .withColumn(\"hora\", date_format(\"Datetime\", \"HH:mm:ss\"))\n    .withColumn(\"hora\", hour(\"Datetime\"))\n)\n\n# Guardar el resultado cacheado para mejorar rendimiento en siguientes operaciones\naccidentesDF.cache()\n\n# Comprobar que se han a\u00f1adido las variables\naccidentesDF.printSchema()"}, {"cell_type": "code", "execution_count": 2, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "214402d3a6c78ca57ae876cb84f7e722", "grade": true, "grade_id": "read_csv_test", "locked": true, "points": 1.5, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.types import DoubleType\nassert(accidentesDF.schema[1].dataType == DoubleType())\nassert(accidentesDF.count() == 251832)\n\nassert(dict(accidentesDF.dtypes)[\"Age_Category\"] == \"string\")\ncollectedDF = accidentesDF.groupBy(\"Age_Category\").count().orderBy(\"count\").collect()\nassert((collectedDF[0][\"count\"] == 22533) & (collectedDF[0][\"Age_Category\"] == \"Anciano\"))\nassert((collectedDF[1][\"count\"] == 57174) & (collectedDF[1][\"Age_Category\"] == \"Adolescente\"))\nassert((collectedDF[2][\"count\"] == 67138) & (collectedDF[2][\"Age_Category\"] == \"Adulto\"))\nassert((collectedDF[3][\"count\"] == 104987) & (collectedDF[3][\"Age_Category\"] == \"Joven\"))\n"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "3ea5104d104e3d22b6d9d8a1eba3b7b7", "grade": false, "grade_id": "cell-b90f5b934eda250e", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 2 (3 puntos)** \n\nPartiendo de `accidentesDF`, queremos pegar (**sin hacer JOIN sino usando agregaciones sobre ventanas**) a cada accidente la siguiente informaci\u00f3n:\n* N\u00famero de accidentes que ha habido *en ese mismo a\u00f1o con esa misma categor\u00eda de veh\u00edculo*, en una nueva columna `total_vehiculo_anio`.\n* N\u00famero de accidentes que ha habido *en ese mismo a\u00f1o con esa misma categor\u00eda de veh\u00edculo y con esa misma situaci\u00f3n (Junction Location)*, en una nueva columna `total_vehiculo_causa_anio`.\n* Porcentaje (en tanto por uno) que supone el segundo dato sobre el primero, en la columna `porc_vehiculo_causa_anio`. Esta columna la podr\u00e1s calcular tras haber calculado las dos anteriores, sin necesidad de utilizar ninguna ventana.\n* Edad promedio de los accidentados *en ese mismo a\u00f1o con esa misma categor\u00eda de veh\u00edculo*, en una nueva columna `media_edad_vehiculo_anio`.\n* Edad promedio de los accidentados *en ese mismo a\u00f1o con esa misma categor\u00eda de veh\u00edculo y con esa misma situaci\u00f3n (Junction_Location)*, en una nueva columna `media_edad_vehiculo_causa_anio`.\n* Desviaci\u00f3n t\u00edpica (funci\u00f3n `F.stddev`) del dato anterior *en ese mismo a\u00f1o con ese mismo tipo de veh\u00edculo y con esa misma situaci\u00f3n (Junction_Location)*, en una nuea columna `stddev_edad_vehiculo_causa_anio`.\n* Guardar el DF resultante en una nueva variable llamada `accidentes_info_agregadaDF`.\n\nPISTA: crear en las variables `ventana_vehiculo_anio` y `ventana_vehiculo_causa_anio` dos ventanas diferentes que definan los dos grupos distintos que intervienen en los c\u00e1lculos anteriores (una de ellos con dos criterios y la otra con tres).\n\n**Revisa el diccionario de variables y su significado para saber qu\u00e9 columnas debes utilizar**."}, {"cell_type": "code", "execution_count": 3, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "2b7e75daf483336d265c2afeeb3b343c", "grade": false, "grade_id": "windows", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col, count, avg, stddev\nfrom pyspark.sql import functions as F\n\n# Definir ventanas\nventana_vehiculo_anio = Window.partitionBy(\"Year\", \"Vehicle_Category\")\nventana_vehiculo_causa_anio = Window.partitionBy(\"Year\", \"Vehicle_Category\", \"Junction_Location\")\n\n# Realizar operaciones en el df\naccidentesDF = (\n    accidentesDF\n    .withColumn(\"total_vehiculo_anio\", count(\"*\").over(ventana_vehiculo_anio))\n    .withColumn(\"total_vehiculo_causa_anio\", count(\"*\").over(ventana_vehiculo_causa_anio))\n    .withColumn(\"porc_vehiculo_causa_anio\", col(\"total_vehiculo_causa_anio\") / col(\"total_vehiculo_anio\"))\n    .withColumn(\"media_edad_vehiculo_anio\", avg(\"Age_of_Driver\").over(ventana_vehiculo_anio))\n    .withColumn(\"media_edad_vehiculo_causa_anio\", avg(\"Age_of_Driver\").over(ventana_vehiculo_causa_anio))\n    .withColumn(\"stddev_edad_vehiculo_causa_anio\", stddev(\"Age_of_Driver\").over(ventana_vehiculo_causa_anio))\n)\n\naccidentes_info_agregadaDF = accidentesDF"}, {"cell_type": "code", "execution_count": 4, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "b6d6e3947a2cac27e9cc0a6f16eddbb9", "grade": true, "grade_id": "windows_tests", "locked": true, "points": 3, "schema_version": 3, "solution": false, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "r = accidentes_info_agregadaDF.select(F.mean(\"total_vehiculo_anio\").alias(\"total_vehiculo_anio\"),\n                                  F.mean(\"total_vehiculo_causa_anio\").alias(\"total_vehiculo_causa_anio\"),\n                                  F.mean(\"porc_vehiculo_causa_anio\").alias(\"porc_vehiculo_causa_anio\"),\n                                  F.mean(\"media_edad_vehiculo_causa_anio\").alias(\"media_edad_vehiculo_causa_anio\"),\n                                 ).first()\nassert(round(r.total_vehiculo_anio, 2) == 33843.98)\nassert(round(r.total_vehiculo_causa_anio, 2) == 8185.52)\nassert(round(r.porc_vehiculo_causa_anio, 2) == 0.25)\nassert(round(r.media_edad_vehiculo_causa_anio, 2) == 3.90)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "0e4076dfd2f060035bc93d59660edd10", "grade": false, "grade_id": "cell-fc88821f19453a51", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 3 (1 punto)** Queremos saber si el tipo de veh\u00edculo est\u00e1 relacionado con la hora del d\u00eda a la que se tienen m\u00e1s accidentes, y si es diferente entre cada tipo de veh\u00edculo. Para ello, partiendo de nuevo de `accidentesDF` \n* Crear un nuevo DF con tantas filas como horas del d\u00eda existen, y tantas columnas como categor\u00edas de veh\u00edculo m\u00e1s una (que ser\u00e1 justamente la hora). En cada casilla, debe contener el n\u00famero de accidentes ocurridos a esa hora del d\u00eda con ese tipo de veh\u00edculo.\n* Ordenar el DF en base a la hora de menor a mayor.\n* Almacenar el DF resultante en la variable `accidentes_hora_vehiculo`."}, {"cell_type": "code", "execution_count": 5, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "fe4b77eccb84199d2ee1209f64f95764", "grade": false, "grade_id": "accidentes_horas_edad", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col, count\n\n# Agregar la columna datetime\naccidentesDF = accidentesDF.withColumn(\"datetime\", col(\"hora\").substr(1, 2))\n\n# Definir la ventana por hora\nventana_hora = Window.partitionBy(\"hora\")\n\n# Crear el df accidentes_hora_vehiculo\naccidentes_hora_vehiculo = (\n    accidentesDF\n    .groupBy(\"hora\")\n    .pivot(\"Vehicle_Category\")\n    .agg(count(\"*\"))\n    .orderBy(\"hora\")\n)"}, {"cell_type": "code", "execution_count": 6, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "74d20d67561b86e9d03d5d6b1463a153", "grade": true, "grade_id": "accidentes_horas_edad_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "acc = accidentes_hora_vehiculo.collect()\nassert(acc[0].hora == 0 and acc[0].Taxi == 324)\nassert(acc[10].hora == 10 and acc[10].Other == 41)\nassert(acc[15].hora == 15 and acc[15].Motorcycle == 1860)\nassert(acc[19].hora == 19 and acc[19].Car == 10886)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "ac7cddd3b4a27d28c49547bbf66191ea", "grade": false, "grade_id": "cell-b7957df90f6199a3", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 4 (1 punto)** Partiendo de la variable `accidentes_hora_vehiculo` creada en el ejercicio anterior, crear un nuevo DF de **una sola fila** y tantas columnas como categor\u00edas de veh\u00edculos (es decir, 6). Debe contener, para cada columna, una *pareja del n\u00famero de accidentes m\u00e1ximo que ocurre a lo largo del d\u00eda, y la hora a la que se produjeron*. Para ello, en lugar de ir aplicando la funci\u00f3n `F.max` a cada columna del DF anterior (dentro de una llamada a `select`), apl\u00edcala en cada momento lo que devuelve la funci\u00f3n `F.struct(nombreColumna, \"hora\"`), es decir, `F.max(F.struct(nombreColumna, \"hora\"))`. De esta forma, estar\u00e1s creando (al vuelo) un objeto columna de parejas, cuyo primer elemento de cada pareja es el n\u00famero total de accidentes indicado en esa columna, y cuyo segundo elemento es la hora del d\u00eda a la que se ha producido. La funci\u00f3n `F.max` aplicada a una columna de tipo parejas tendr\u00e1 en cuenta, por defecto, solamente el primer elemento de cada pareja para ordenar, as\u00ed que escoger\u00e1 la pareja que tiene un mayor n\u00famero de accidentes ya que ese valor es el primer elemento de cada pareja, pero lo mostrar\u00e1 como pareja, con lo que veremos la hora del d\u00eda a la que va aparejado ese n\u00famero de accidentes.\n\nCada columna de pares mostrada por F.max debe renombrarse exactamente con el nombre de la categor\u00eda de veh\u00edculo a la que corresponde esa pareja. \n\nEl DF resultante debe quedar guardado en la variable `hora_max_accidentes_vehiculo_df`\n\nPISTA: la soluci\u00f3n es simplemente una operaci\u00f3n `select` que incluye dentro la creaci\u00f3n de 6 columnas al vuelo haciendo 6 llamadas a la funci\u00f3n `F.max(F.struct(..., \"hora\"))`, y haciendo `alias` sobre el objeto columna devuelto por cada una de estas llamadas, para que cada una de esas 6 columnas creadas al vuelo se llame igual que su categor\u00eda de veh\u00edculo."}, {"cell_type": "code", "execution_count": 7, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "a1695635e6d3eaf0de79f854590ffe8f", "grade": false, "grade_id": "parejas", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [], "source": "# L\u00cdNEA EVALUABLE, NO RENOMBRAR LAS VARIABLES\nfrom pyspark.sql.functions import col, struct, max\n\n# Obtener las columnas de veh\u00edculo\nvehiculo_columnas = accidentes_hora_vehiculo.columns[1:]\n\n# Obtener la hora con el m\u00e1ximo de accidentes para cada veh\u00edculo\ncolumn_exprs = [\n    max(struct(col(col_name), col(\"hora\"))).alias(col_name)\n    for col_name in vehiculo_columnas\n]\n\n# Agregar las expresiones al df\nhora_max_accidentes_vehiculo_df = accidentes_hora_vehiculo.agg(*column_exprs)"}, {"cell_type": "code", "execution_count": 8, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "12eac985a78396cbb431c28fb982cb5f", "grade": true, "grade_id": "parejas_test", "locked": true, "points": 1, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(len(hora_max_accidentes_vehiculo_df.columns) == 6)\nassert(sum([1 for c in [\"Bus/minibus\", \"Car\", \"Motorcycle\", \"Other\", \"Taxi\", \"Van\"]\n          if c in hora_max_accidentes_vehiculo_df.columns]) == 6)\nr2 = hora_max_accidentes_vehiculo_df.first()\nassert(r2[\"Bus/minibus\"][0] == 56 and r2[\"Bus/minibus\"][1] == 15)\nassert(r2[\"Car\"][0] == 19961 and r2[\"Car\"][1] == 17)\nassert(r2[\"Motorcycle\"][0] == 2751 and r2[\"Motorcycle\"][1] == 17)\nassert(r2[\"Other\"][0] == 64 and r2[\"Other\"][1] == 13)\nassert(r2[\"Taxi\"][0] == 389 and r2[\"Taxi\"][1] == 16)\nassert(r2[\"Van\"][0] == 1233 and r2[\"Van\"][1] == 16)"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "fafa36482bcec7ffb2e3cdcab5432e1d", "grade": false, "grade_id": "cell-a71a6b17b1e0d613", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 5 (2 puntos)** Vamos a preprocesar algunas variables para prepararlas para un posible algoritmo predictivo. Partiendo de `accidentesDF` se pide:\n* Crear en la variable `journey_purpose_indexer` un StringIndexer para la variable \"Driver_Journey_Purpose\" y que cree una nueva columna de salida `purpose_indexed`. Debe ser capaz de lidiar con etiquetas nunca vistas a la hora de hacer la codificaci\u00f3n de un nuevo dataset (que no se eliminen dichas filas ni tampoco salte un error).\n* Crear en la variable `cars_involved_binarizer` un Binarizer de la variable `Number_of_Vehicles` que tenga `threshold=2.5` puesto que en la mayor\u00eda de los accidentes est\u00e1n involucrados 1 o 2 coches. Queremos pasarla a una variable binaria donde el 0.0 represente justamente que ha habido 1 o 2 coches involucrados, y el 1.0 represente que ha habido 3 o m\u00e1s coches involucrados. El binarizador debe crear como salida una nueva columna llamada `number_vehicles_binarized`\n* Crear en la variable `vector_assembler` un VectorAssembler que colapse en una nueva columna de tipo vector las columnas `purpose_indexed`, `number_vehicles_binarized` y `Speed_limit`. La nueva columna debe llamarse `features`. \n* Crear en la variable `pipeline` un pipeline que contenga **exclusivamente** las tres etapas anteriores. **NO DEBE CONTENER NING\u00daN ALGORITMO PREDICTIVO**.\n* \"Entrenar\" ese pipeline con el DF `accidentesDF` y guardar el resultado en la variable `pipeline_model`. **No debe hacerse ning\u00fan tipo de divisi\u00f3n de los datos en entrenamiento y test**. Aunque el m\u00e9todo sea \"entrenar\", en realidad s\u00f3lo estamos ajustando etapas de pre-procesamiento."}, {"cell_type": "code", "execution_count": 9, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "a3a1d5b178979e3f47a9983bc8fa1565", "grade": false, "grade_id": "numero_categorias", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.ml.feature import StringIndexer, Binarizer, VectorAssembler\nfrom pyspark.ml import Pipeline\n\n# Definir los indexadores y binarizadores\njourney_purpose_indexer = StringIndexer(inputCol=\"Driver_Journey_Purpose\", outputCol=\"purpose_indexed\", handleInvalid=\"keep\")\ncars_involved_binarizer = Binarizer(inputCol=\"Number_of_Vehicles\", outputCol=\"number_vehicles_binarized\", threshold=2.5)\n\nvector_assembler = VectorAssembler(inputCols=[\"purpose_indexed\", \"number_vehicles_binarized\", \"Speed_limit\"],\n                                   outputCol=\"features\")\n\n# Crear el pipeline\npipeline = Pipeline(stages=[journey_purpose_indexer, cars_involved_binarizer, vector_assembler])\n\n# Ajustar el modelo del pipeline al df\npipeline_model = pipeline.fit(accidentesDF)\n"}, {"cell_type": "code", "execution_count": 10, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "4ab721356e0aa5734558f0999cab364d", "grade": true, "grade_id": "numero_categorias_test", "locked": true, "points": 2, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "from pyspark.ml.feature import StringIndexer, Binarizer, VectorAssembler\nfrom pyspark.ml import Pipeline, PipelineModel\n\nassert(isinstance(journey_purpose_indexer, StringIndexer))\nassert(journey_purpose_indexer.getInputCol() == \"Driver_Journey_Purpose\" and \n       journey_purpose_indexer.getOutputCol() == \"purpose_indexed\" and\n       journey_purpose_indexer.getHandleInvalid() == \"keep\")\n\nassert(isinstance(cars_involved_binarizer, Binarizer))\nassert(cars_involved_binarizer.getInputCol() == \"Number_of_Vehicles\" and \n       cars_involved_binarizer.getOutputCol() == \"number_vehicles_binarized\" and\n       cars_involved_binarizer.getThreshold() == 2.5)\n\nassert(isinstance(pipeline, Pipeline))\nassert(len(pipeline.getStages()) == 3)             # el pipeline debe tener solamente tres etapas\nassert(journey_purpose_indexer in pipeline.getStages() \n       and cars_involved_binarizer in pipeline.getStages() \n       and vector_assembler in pipeline.getStages())\n\nassert(isinstance(pipeline_model, PipelineModel))"}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "markdown", "checksum": "c20156c0fe97d2976dfa3b10f3693be7", "grade": false, "grade_id": "cell-27e00b5fb188db3a", "locked": true, "schema_version": 3, "solution": false, "task": false}}, "source": "**Ejercicio 6 (1.5 puntos)** Queremos ver cu\u00e1l es la forma de transporte involucrada en m\u00e1s accidentes en cada regi\u00f3n de Reino Unido. Para ello, partiendo de `accidentesDF` se pide:\n\n* Crear un DF con tantas filas como Regiones distintas existen y tantas columnas como categor\u00edas de veh\u00edculo m\u00e1s una (la de la regi\u00f3n, que estar\u00e1 a la izquierda). En cada casilla debe calcularse una **tripleta** (columna de tipo estructura, que se crea con `F.struct(col1, col2, col3)`) formada por:\n  * N\u00famero de accidentes en esa regi\u00f3n y tipo veh\u00edculo,\n  * Edad media del conductor redondeada a 2 cifras decimales, y\n  * N\u00famero medio de coches involucrados redondeado a 2 cifras decimales). \n* Ordenar el DF alfab\u00e9ticamente de menor a mayor en base a la columna `\"Region\"`\n* Guardar el DF resultante en la variable `numero_edad_coches_df`.\n* Para visualizarlo mejor, y puesto que el tama\u00f1o del DF que hemos obtenido como resultado est\u00e1 acotado por el n\u00famero de regiones distintas existentes y por el n\u00famero de categor\u00edas de veh\u00edculos existentes, pasar dicho DF a un dataframe de Pandas en la variable `numero_edad_coches_pd` y mostrarlo por pantalla.\n\nPISTA: para construir la columna de tipo estructura dentro de la funci\u00f3n `agg(...)` se puede utilizar `F.struct(F.funcionagregacion(...), F.funcionagregacion(...), F.funcionagregacion(...))`. \n\nPISTA: en vez de pasarle a la funci\u00f3n `F.struct` directamente la columna resultante de la agregaci\u00f3n, p\u00e1sale en caso necesario `F.round(F.nombrefuncion(...), 2)` para que ya est\u00e9 redondeada.\n\nEn el resultado puedes observar fen\u00f3menos como por ejemplo: \n* Los conductores de autob\u00fas son los que en promedio tienen siempre m\u00e1s edad, mientras que los de moto son los m\u00e1s j\u00f3venes, como era de esperar. \n* Los accidentes de moto son los que menos coches involucran en promedio, en torno a 1.80, lo que quiere decir que hay muchos accidentes que los tiene el propio conductor sin que intervenga otro veh\u00edculo (condiciones atmosf\u00e9ricas, etc). Los de Taxi parecen estar bastante por debajo que los accidentes de coche, lo que indica que, mientras que un accidente de coche con frecuencia implica la interacci\u00f3n con otro veh\u00edculo, en los taxis hay a\u00fan bastantes accidentes donde no necesariamente hay otro veh\u00edculo implicado y por eso el promedio todav\u00eda no se acerca tanto a 2.\n* Es llamativo que en Gales haya unos promedios tan bajos en el n\u00famero de veh\u00edculos involucrados en todas las categor\u00edas, en especial en los accidentes de coche y moto, lo que indica que muchos accidentes se producen sin causa de otro veh\u00edculo. Puede tener relaci\u00f3n directa con que la edad media de los conductores es bastante superior a la de otros medios, y por eso son propensos a tener un accidente por mala conducci\u00f3n o relacionado con las facultades f\u00edsicas o cognitivas del conductor."}, {"cell_type": "code", "execution_count": 11, "metadata": {"deletable": false, "nbgrader": {"cell_type": "code", "checksum": "390b166c02a0c1d81753a9ac3a404fc8", "grade": false, "grade_id": "tripletas", "locked": false, "schema_version": 3, "solution": true, "task": false}}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Region</th>\n      <th>Bus/minibus</th>\n      <th>Car</th>\n      <th>Motorcycle</th>\n      <th>Other</th>\n      <th>Taxi</th>\n      <th>Van</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>East England</td>\n      <td>(44, 4.34, 1.77)</td>\n      <td>(22813, 3.95, 2.0)</td>\n      <td>(2753, 3.17, 1.8)</td>\n      <td>(61, 4.02, 1.87)</td>\n      <td>(424, 4.43, 1.89)</td>\n      <td>(1470, 3.91, 2.12)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>East Midlands</td>\n      <td>(39, 4.95, 1.85)</td>\n      <td>(17191, 3.91, 1.96)</td>\n      <td>(2084, 3.29, 1.79)</td>\n      <td>(48, 4.44, 1.92)</td>\n      <td>(362, 4.36, 1.83)</td>\n      <td>(1117, 3.98, 2.06)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>London</td>\n      <td>(28, 4.71, 1.75)</td>\n      <td>(24015, 3.9, 1.92)</td>\n      <td>(5658, 3.22, 1.82)</td>\n      <td>(56, 3.89, 1.84)</td>\n      <td>(1671, 4.63, 1.79)</td>\n      <td>(2212, 3.88, 1.91)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>North East England</td>\n      <td>(52, 4.46, 1.85)</td>\n      <td>(9272, 3.96, 1.96)</td>\n      <td>(742, 3.46, 1.79)</td>\n      <td>(32, 3.81, 2.06)</td>\n      <td>(279, 4.12, 1.67)</td>\n      <td>(639, 3.95, 2.0)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>North West England</td>\n      <td>(66, 4.7, 1.88)</td>\n      <td>(25783, 4.01, 1.97)</td>\n      <td>(2724, 3.29, 1.82)</td>\n      <td>(66, 4.47, 1.98)</td>\n      <td>(1070, 4.33, 1.75)</td>\n      <td>(1387, 3.91, 2.01)</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Scotland</td>\n      <td>(2, 4.0, 1.5)</td>\n      <td>(198, 4.24, 1.45)</td>\n      <td>(65, 4.77, 1.25)</td>\n      <td>(2, 7.0, 1.5)</td>\n      <td>(1, 5.0, 1.0)</td>\n      <td>(26, 4.0, 1.46)</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>South East England</td>\n      <td>(71, 4.96, 1.87)</td>\n      <td>(39410, 4.03, 2.02)</td>\n      <td>(5281, 3.28, 1.8)</td>\n      <td>(145, 3.81, 2.0)</td>\n      <td>(787, 4.42, 1.78)</td>\n      <td>(2548, 3.92, 2.1)</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>South West England</td>\n      <td>(37, 4.84, 1.81)</td>\n      <td>(21144, 4.11, 1.99)</td>\n      <td>(2802, 3.17, 1.88)</td>\n      <td>(67, 4.49, 2.0)</td>\n      <td>(317, 4.58, 1.74)</td>\n      <td>(1236, 3.97, 2.03)</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Wales</td>\n      <td>(1, 5.0, 2.0)</td>\n      <td>(282, 4.23, 1.73)</td>\n      <td>(79, 4.35, 1.51)</td>\n      <td>(2, 2.5, 1.5)</td>\n      <td>(0, None, None)</td>\n      <td>(29, 3.52, 1.83)</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Wast Midlands</td>\n      <td>(56, 4.7, 1.91)</td>\n      <td>(20933, 3.85, 1.97)</td>\n      <td>(2168, 3.16, 1.88)</td>\n      <td>(52, 3.96, 1.92)</td>\n      <td>(607, 4.14, 1.79)</td>\n      <td>(1436, 3.91, 2.03)</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Yorkshire and the Humber</td>\n      <td>(75, 4.67, 1.69)</td>\n      <td>(23049, 3.93, 1.96)</td>\n      <td>(2537, 3.27, 1.82)</td>\n      <td>(75, 3.92, 1.99)</td>\n      <td>(742, 4.15, 1.75)</td>\n      <td>(1412, 3.92, 2.02)</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "                      Region       Bus/minibus                  Car  \\\n0               East England  (44, 4.34, 1.77)   (22813, 3.95, 2.0)   \n1              East Midlands  (39, 4.95, 1.85)  (17191, 3.91, 1.96)   \n2                     London  (28, 4.71, 1.75)   (24015, 3.9, 1.92)   \n3         North East England  (52, 4.46, 1.85)   (9272, 3.96, 1.96)   \n4         North West England   (66, 4.7, 1.88)  (25783, 4.01, 1.97)   \n5                   Scotland     (2, 4.0, 1.5)    (198, 4.24, 1.45)   \n6         South East England  (71, 4.96, 1.87)  (39410, 4.03, 2.02)   \n7         South West England  (37, 4.84, 1.81)  (21144, 4.11, 1.99)   \n8                      Wales     (1, 5.0, 2.0)    (282, 4.23, 1.73)   \n9              Wast Midlands   (56, 4.7, 1.91)  (20933, 3.85, 1.97)   \n10  Yorkshire and the Humber  (75, 4.67, 1.69)  (23049, 3.93, 1.96)   \n\n            Motorcycle             Other                Taxi  \\\n0    (2753, 3.17, 1.8)  (61, 4.02, 1.87)   (424, 4.43, 1.89)   \n1   (2084, 3.29, 1.79)  (48, 4.44, 1.92)   (362, 4.36, 1.83)   \n2   (5658, 3.22, 1.82)  (56, 3.89, 1.84)  (1671, 4.63, 1.79)   \n3    (742, 3.46, 1.79)  (32, 3.81, 2.06)   (279, 4.12, 1.67)   \n4   (2724, 3.29, 1.82)  (66, 4.47, 1.98)  (1070, 4.33, 1.75)   \n5     (65, 4.77, 1.25)     (2, 7.0, 1.5)       (1, 5.0, 1.0)   \n6    (5281, 3.28, 1.8)  (145, 3.81, 2.0)   (787, 4.42, 1.78)   \n7   (2802, 3.17, 1.88)   (67, 4.49, 2.0)   (317, 4.58, 1.74)   \n8     (79, 4.35, 1.51)     (2, 2.5, 1.5)     (0, None, None)   \n9   (2168, 3.16, 1.88)  (52, 3.96, 1.92)   (607, 4.14, 1.79)   \n10  (2537, 3.27, 1.82)  (75, 3.92, 1.99)   (742, 4.15, 1.75)   \n\n                   Van  \n0   (1470, 3.91, 2.12)  \n1   (1117, 3.98, 2.06)  \n2   (2212, 3.88, 1.91)  \n3     (639, 3.95, 2.0)  \n4   (1387, 3.91, 2.01)  \n5      (26, 4.0, 1.46)  \n6    (2548, 3.92, 2.1)  \n7   (1236, 3.97, 2.03)  \n8     (29, 3.52, 1.83)  \n9   (1436, 3.91, 2.03)  \n10  (1412, 3.92, 2.02)  "}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nimport pandas as pd\n\nspark = SparkSession.builder.getOrCreate()\nspark = SparkSession.builder.config(\"spark.sql.debug.maxToStringFields\", \"1000\").getOrCreate()\n\n# Crear el pivotedf\npivotedDF = accidentesDF.groupBy(\"Region\").pivot(\"Vehicle_Category\").agg(\n    F.struct(\n        F.count(\"*\").alias(\"Numero_Accidentes\"),\n        F.round(F.avg(\"Age_of_Driver\"), 2).alias(\"Edad_Media_Conductor\"),\n        F.round(F.avg(\"Number_of_Vehicles\"), 2).alias(\"Numero_Medio_Coches\")))\n\ncolumns = [\"Region\"] + [col for col in pivotedDF.columns if col != \"Region\"]\n\nnumero_edad_coches_df = (\n    pivotedDF\n    .select(columns)\n    .orderBy(\"Region\")\n)\n\nnumero_edad_coches_pd = numero_edad_coches_df.toPandas()\n\n# Mostrar df por pantalla\nnumero_edad_coches_pd"}, {"cell_type": "code", "execution_count": 12, "metadata": {"deletable": false, "editable": false, "nbgrader": {"cell_type": "code", "checksum": "14de8030c7478c1104f20a9840327041", "grade": true, "grade_id": "tripletas_test", "locked": true, "points": 1.5, "schema_version": 3, "solution": false, "task": false}}, "outputs": [], "source": "assert(len(numero_edad_coches_df.columns) == 7)\nassert(sum([1 for c in [\"Region\", \"Bus/minibus\", \"Car\", \"Motorcycle\", \"Other\", \"Taxi\", \"Van\"]\n          if c in numero_edad_coches_df.columns]) == 7)\nr = numero_edad_coches_df.first()\nassert(r.Region == \"East England\" and r.Other == (61, 4.02, 1.87))\nassert(numero_edad_coches_pd.shape == (11, 7))\nassert(numero_edad_coches_pd.loc[0, \"Car\"] == (22813, 3.95, 2.0))\nassert(numero_edad_coches_pd.loc[7, \"Motorcycle\"] == (2802, 3.17, 1.88))\nassert(numero_edad_coches_pd.loc[10, \"Van\"] == (1412, 3.92, 2.02))"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.15"}}, "nbformat": 4, "nbformat_minor": 4}